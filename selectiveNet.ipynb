{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import BatchNormalization, Add, Subtract, Concatenate, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input, LSTM, Embedding, Bidirectional, Flatten\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.python.keras.constraints import maxnorm\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from selectivnet_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnBilstm:\n",
    "    def __init__(self, train=True, filename=\"weight-enigma.h5\", coverage=0.8, alpha=0.5, baseline=False):\n",
    "        self.lamda = coverage\n",
    "        self.alpha = alpha\n",
    "        self.mc_dropout_rate = K.variable(value=0)\n",
    "        self.num_classes = 3\n",
    "        self.weight_decay = 0.0005\n",
    "        self.valid_per = 0.2\n",
    "        \n",
    "        self.x_shape = 100 #padding\n",
    "        self._load_data()\n",
    "\n",
    "        self.filename = filename\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        if baseline:\n",
    "            self.alpha = 0\n",
    "\n",
    "        if train:\n",
    "            self.model = self.train(self.model)\n",
    "        else:\n",
    "            self.model.load_weights(\"checkpoints/{}\".format(self.filename))\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
    "        weight_decay = self.weight_decay\n",
    "        basic_dropout_rate = 0.1\n",
    "        input = Input(shape=self.x_shape)\n",
    "        \n",
    "        embedding_size = 128\n",
    "        print(\"Vocab Size:{}\".format(self.vocab_size))\n",
    "        print(\"Embedding Size:{}\".format(embedding_size))\n",
    "\n",
    "        # Keras Embedding layer with Word2Vec weights initialization\n",
    "        curr = Embedding(input_dim=self.vocab_size, output_dim=embedding_size)(input)\n",
    "        curr = Bidirectional(LSTM(128, return_sequences=True, dropout=basic_dropout_rate + 0.2, recurrent_dropout=0.1, kernel_regularizer=regularizers.l2(weight_decay)))(curr)\n",
    "        curr = Activation('relu')(curr)\n",
    "        curr = BatchNormalization()(curr)\n",
    "        \n",
    "        curr = Bidirectional(LSTM(128, return_sequences=False, dropout=basic_dropout_rate + 0.2, recurrent_dropout=0.1, kernel_regularizer=regularizers.l2(weight_decay)))(curr)\n",
    "        curr = Activation('relu')(curr)\n",
    "        curr = BatchNormalization()(curr)\n",
    "\n",
    "        \n",
    "        curr = Dense(128, kernel_regularizer=regularizers.l2(weight_decay))(curr)\n",
    "        curr = Activation('relu')(curr)\n",
    "        curr = Dropout(basic_dropout_rate + 0.2)(curr)\n",
    "        curr = BatchNormalization()(curr)\n",
    "        \n",
    "        curr = Dense(64, kernel_regularizer=regularizers.l2(weight_decay))(curr)\n",
    "        curr = Activation('relu')(curr)\n",
    "        curr = BatchNormalization()(curr)\n",
    "        curr = Lambda(lambda x: K.dropout(x, level=self.mc_dropout_rate))(curr)\n",
    "        \n",
    "        # classification head (f)\n",
    "        curr1 = Dense(self.num_classes, activation='softmax')(curr)\n",
    "\n",
    "        # selection head (g)\n",
    "        curr2 = Dense(64, kernel_regularizer=regularizers.l2(weight_decay))(curr)\n",
    "        curr2 = Activation('relu')(curr2)\n",
    "        curr2 = BatchNormalization()(curr2)\n",
    "        # this normalization is identical to initialization of batchnorm gamma to 1/10\n",
    "        curr2 = Lambda(lambda x: x / 10)(curr2)\n",
    "        curr2 = Dense(1, activation='sigmoid')(curr2)\n",
    "        selective_output = Concatenate(axis=1, name=\"selective_head\")([curr1, curr2])\n",
    "\n",
    "        # auxiliary head (h)\n",
    "        auxiliary_output = Dense(self.num_classes, activation='softmax', name=\"classification_head\")(curr)\n",
    "\n",
    "        model = Model(inputs=input, outputs=[selective_output, auxiliary_output])\n",
    "\n",
    "        self.input = input\n",
    "        self.model_embeding = Model(inputs=input, outputs=curr)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def predict(self, x=None, batch_size=128):\n",
    "        if x is None:\n",
    "            x = self.x_test\n",
    "        return self.model.predict(x, batch_size)\n",
    "\n",
    "    def predict_embedding(self, x=None, batch_size=128):\n",
    "        if x is None:\n",
    "            x = self.x_test\n",
    "        return self.model_embeding.predict(x, batch_size)\n",
    "\n",
    "    def selective_risk_at_coverage(self, coverage, mc=False):\n",
    "        _, pred = self.predict()\n",
    "\n",
    "        if mc:\n",
    "            sr = np.max(pred, 1)\n",
    "        else:\n",
    "            sr = self.mc_dropout()\n",
    "        sr_sorted = np.sort(sr)\n",
    "        threshold = sr_sorted[pred.shape[0] - int(coverage * pred.shape[0])]\n",
    "        covered_idx = sr > threshold\n",
    "        selective_acc = np.mean(np.argmax(pred[covered_idx], 1) == np.argmax(self.y_test[covered_idx], 1))\n",
    "        return selective_acc\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open('models/tokeniser_2020_7_14.pickle', 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "            \n",
    "        with open('train_data/tokenized_data_2020_7_14.pickle', 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            \n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        \n",
    "        data_padded = []\n",
    "        y = []\n",
    "        for i,j in data:\n",
    "            data_padded.append(i)\n",
    "            y.append(j)\n",
    "        y=np.array(y)\n",
    "            \n",
    "        data_padded = pad_sequences(data_padded, maxlen=self.x_shape, padding='pre', truncating='pre')\n",
    "        \n",
    "        total_samples = data_padded.shape[0]\n",
    "        n_val = int((self.valid_per * total_samples) / 128)*128 + 128\n",
    "        n_train = total_samples - n_val\n",
    "\n",
    "        random_i = random.sample(range(total_samples), total_samples)\n",
    "        self.x_train = data_padded[random_i[:n_train]]\n",
    "        self.y_train = tf.keras.utils.to_categorical(y[random_i[:n_train]], self.num_classes + 1)\n",
    "        self.x_test = data_padded[random_i[n_train:]]\n",
    "        self.y_test = tf.keras.utils.to_categorical(y[random_i[n_train:]], self.num_classes + 1)\n",
    "        print(\"Train Shapes - X: {} - Y: {}\".format(self.x_train.shape, self.y_train.shape))\n",
    "        print(\"Val Shapes - X: {} - Y: {}\".format(self.x_test.shape, self.y_test.shape))\n",
    "\n",
    "    def train(self, model):\n",
    "        c = self.lamda\n",
    "        lamda = 32\n",
    "\n",
    "        def selective_loss(y_true, y_pred):\n",
    "            loss = K.categorical_crossentropy(\n",
    "                K.repeat_elements(y_pred[:, -1:], self.num_classes, axis=1) * y_true[:, :-1],\n",
    "                y_pred[:, :-1]) + lamda * K.maximum(-K.mean(y_pred[:, -1]) + c, 0) ** 2\n",
    "            return loss\n",
    "\n",
    "        def selective_acc(y_true, y_pred):\n",
    "            g = K.cast(K.greater(y_pred[:, -1], 0.5), K.floatx())\n",
    "            temp1 = K.sum(\n",
    "                (g) * K.cast(K.equal(K.argmax(y_true[:, :-1], axis=-1), K.argmax(y_pred[:, :-1], axis=-1)), K.floatx()))\n",
    "            temp1 = temp1 / K.sum(g)\n",
    "            return K.cast(temp1, K.floatx())\n",
    "\n",
    "        def coverage(y_true, y_pred):\n",
    "            g = K.cast(K.greater(y_pred[:, -1], 0.5), K.floatx())\n",
    "            return K.mean(g)\n",
    "\n",
    "\n",
    "\n",
    "        # training parameters\n",
    "        batch_size = 128\n",
    "        maxepoches = 10\n",
    "        learning_rate = 0.1\n",
    "\n",
    "        lr_decay = 1e-6\n",
    "\n",
    "        lr_drop = 25\n",
    "\n",
    "        def lr_scheduler(epoch):\n",
    "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "\n",
    "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "        # optimization details\n",
    "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "\n",
    "        model.compile(loss=[selective_loss, 'categorical_crossentropy'], loss_weights=[self.alpha, 1 - self.alpha],\n",
    "                      optimizer=sgd, metrics=['accuracy', selective_acc, coverage], experimental_run_tf_function=False)\n",
    "        \n",
    "        historytemp = model.fit(self.x_train, [self.y_train, self.y_train[:, :-1]], batch_size=batch_size,\n",
    "                                          epochs=maxepoches, callbacks=[reduce_lr],\n",
    "                                          validation_data=(self.x_test, [self.y_test, self.y_test[:, :-1]]))\n",
    "\n",
    "        with open(\"checkpoints/{}_history.pkl\".format(self.filename[:-3]), 'wb') as handle:\n",
    "            pickle.dump(historytemp.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        model.save_weights(\"checkpoints/{}\".format(self.filename))\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shapes - X: (75036, 100) - Y: (75036, 4)\n",
      "Val Shapes - X: (18816, 100) - Y: (18816, 4)\n",
      "Vocab Size:114557\n",
      "Embedding Size:128\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 128)     14663296    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 256)     263168      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 100, 256)     0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 100, 256)     1024        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          394240      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          32896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64)           256         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 64)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           256         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            195         lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            65          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "selective_head (Concatenate)    (None, 4)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classification_head (Dense)     (None, 3)            195         lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 15,369,543\n",
      "Trainable params: 15,368,007\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 75036 samples, validate on 18816 samples\n",
      "Epoch 1/10\n",
      "75036/75036 [==============================] - 220s 3ms/sample - loss: 1.4785 - selective_head_loss: 0.8809 - classification_head_loss: 1.0841 - selective_head_accuracy: 0.0099 - selective_head_selective_acc: 0.3983 - selective_head_coverage: 0.9993 - classification_head_accuracy: 0.3958 - classification_head_selective_acc: nan - classification_head_coverage: 0.0935 - val_loss: 1.0741 - val_selective_head_loss: 0.7085 - val_classification_head_loss: 0.8699 - val_selective_head_accuracy: 0.1520 - val_selective_head_selective_acc: 0.5959 - val_selective_head_coverage: 1.0000 - val_classification_head_accuracy: 0.5965 - val_classification_head_selective_acc: 0.8489 - val_classification_head_coverage: 0.5188\n",
      "Epoch 2/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.6454 - selective_head_loss: 0.3713 - classification_head_loss: 0.5347 - selective_head_accuracy: 0.3898 - selective_head_selective_acc: 0.7829 - selective_head_coverage: 0.8912 - classification_head_accuracy: 0.7535 - classification_head_selective_acc: 0.9498 - classification_head_coverage: 0.3618 - val_loss: 0.5095 - val_selective_head_loss: 0.3192 - val_classification_head_loss: 0.4331 - val_selective_head_accuracy: 0.6882 - val_selective_head_selective_acc: 0.8816 - val_selective_head_coverage: 0.9304 - val_classification_head_accuracy: 0.8531 - val_classification_head_selective_acc: 0.9883 - val_classification_head_coverage: 0.4175\n",
      "Epoch 3/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.2322 - selective_head_loss: 0.0764 - classification_head_loss: 0.1894 - selective_head_accuracy: 0.8440 - selective_head_selective_acc: 0.9809 - selective_head_coverage: 0.8132 - classification_head_accuracy: 0.9360 - classification_head_selective_acc: 0.9894 - classification_head_coverage: 0.3736 - val_loss: 0.1746 - val_selective_head_loss: 0.0582 - val_classification_head_loss: 0.1384 - val_selective_head_accuracy: 0.9066 - val_selective_head_selective_acc: 0.9855 - val_selective_head_coverage: 0.8666 - val_classification_head_accuracy: 0.9548 - val_classification_head_selective_acc: 0.9917 - val_classification_head_coverage: 0.3918\n",
      "Epoch 4/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.1367 - selective_head_loss: 0.0322 - classification_head_loss: 0.1242 - selective_head_accuracy: 0.9446 - selective_head_selective_acc: 0.9940 - selective_head_coverage: 0.8176 - classification_head_accuracy: 0.9589 - classification_head_selective_acc: 0.9682 - classification_head_coverage: 0.3756 - val_loss: 0.1540 - val_selective_head_loss: 0.0962 - val_classification_head_loss: 0.1203 - val_selective_head_accuracy: 0.9571 - val_selective_head_selective_acc: 0.9959 - val_selective_head_coverage: 0.7745 - val_classification_head_accuracy: 0.9609 - val_classification_head_selective_acc: 0.9931 - val_classification_head_coverage: 0.3664\n",
      "Epoch 5/10\n",
      "75008/75036 [============================>.] - ETA: 0s - loss: 0.0981 - selective_head_loss: 0.0196 - classification_head_loss: 0.0978 - selective_head_accuracy: 0.9640 - selective_head_selective_acc: 0.9966 - selective_head_coverage: 0.8228 - classification_head_accuracy: 0.9682 - classification_head_selective_acc: 0.9490 - classification_head_coverage: 0.3767Epoch 6/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.0835 - selective_head_loss: 0.0155 - classification_head_loss: 0.0873 - selective_head_accuracy: 0.9668 - selective_head_selective_acc: 0.9973 - selective_head_coverage: 0.8263 - classification_head_accuracy: 0.9710 - classification_head_selective_acc: 0.9771 - classification_head_coverage: 0.3762 - val_loss: 0.0901 - val_selective_head_loss: 0.0310 - val_classification_head_loss: 0.0896 - val_selective_head_accuracy: 0.9692 - val_selective_head_selective_acc: 0.9953 - val_selective_head_coverage: 0.8253 - val_classification_head_accuracy: 0.9725 - val_classification_head_selective_acc: 0.9512 - val_classification_head_coverage: 0.3757\n",
      "Epoch 7/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.0738 - selective_head_loss: 0.0137 - classification_head_loss: 0.0788 - selective_head_accuracy: 0.9704 - selective_head_selective_acc: 0.9979 - selective_head_coverage: 0.8268 - classification_head_accuracy: 0.9743 - classification_head_selective_acc: 0.9657 - classification_head_coverage: 0.3764 - val_loss: 0.0917 - val_selective_head_loss: 0.0326 - val_classification_head_loss: 0.0959 - val_selective_head_accuracy: 0.9713 - val_selective_head_selective_acc: 0.9961 - val_selective_head_coverage: 0.8153 - val_classification_head_accuracy: 0.9731 - val_classification_head_selective_acc: 0.9885 - val_classification_head_coverage: 0.3932\n",
      "Epoch 8/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.0651 - selective_head_loss: 0.0099 - classification_head_loss: 0.0704 - selective_head_accuracy: 0.9753 - selective_head_selective_acc: 0.9985 - selective_head_coverage: 0.8262 - classification_head_accuracy: 0.9770 - classification_head_selective_acc: 0.9300 - classification_head_coverage: 0.3767 - val_loss: 0.0857 - val_selective_head_loss: 0.0258 - val_classification_head_loss: 0.0978 - val_selective_head_accuracy: 0.9631 - val_selective_head_selective_acc: 0.9976 - val_selective_head_coverage: 0.8180 - val_classification_head_accuracy: 0.9633 - val_classification_head_selective_acc: 0.9030 - val_classification_head_coverage: 0.3810\n",
      "Epoch 9/10\n",
      "75036/75036 [==============================] - 217s 3ms/sample - loss: 0.0608 - selective_head_loss: 0.0091 - classification_head_loss: 0.0672 - selective_head_accuracy: 0.9770 - selective_head_selective_acc: 0.9985 - selective_head_coverage: 0.8281 - classification_head_accuracy: 0.9783 - classification_head_selective_acc: 0.9405 - classification_head_coverage: 0.3771 - val_loss: 0.0687 - val_selective_head_loss: 0.0224 - val_classification_head_loss: 0.0695 - val_selective_head_accuracy: 0.9794 - val_selective_head_selective_acc: 0.9965 - val_selective_head_coverage: 0.8786 - val_classification_head_accuracy: 0.9800 - val_classification_head_selective_acc: 0.9931 - val_classification_head_coverage: 0.3820\n",
      "Epoch 10/10\n",
      "75008/75036 [============================>.] - ETA: 0s - loss: 0.0559 - selective_head_loss: 0.0088 - classification_head_loss: 0.0614 - selective_head_accuracy: 0.9769 - selective_head_selective_acc: 0.9987 - selective_head_coverage: 0.8267 - classification_head_accuracy: 0.9801 - classification_head_selective_acc: 0.9296 - classification_head_coverage: 0.3769"
     ]
    }
   ],
   "source": [
    "init_model = SnBilstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
